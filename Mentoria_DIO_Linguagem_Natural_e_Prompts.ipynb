{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amos-fernandes/machine-learning-bairesdev-dio/blob/dev/Mentoria_DIO_Linguagem_Natural_e_Prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting, Extraindo a Polpa da Fruta\n",
        "\n",
        "*Me. Guilherme D. F. Silva, Machine Learning Engineer at [BairesDev](https://www.bairesdev.com)*.\n",
        "\n",
        "*PhD. Henrique P. Gomide, Machine Learning Engineer at [BairesDev](https://www.bairesdev.com)*.\n",
        "\n",
        "<br>\n",
        "\n",
        "Como utilizar diferentes t√©cnicas de prompting para melhorar a performance de LLMs e permitir que elas solucionem problemas mais complexos."
      ],
      "metadata": {
        "id": "SAPraac0tg6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Importando bibliotecas e acessando a API [Groq](https://console.groq.com)."
      ],
      "metadata": {
        "id": "sipdzAN9uKWz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5s9UiVfSMVn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_KEY = userdata.get(\"GROQ_KEY\")\n",
        "\n",
        "API_URL = 'https://api.groq.com/openai/v1/chat/completions'\n",
        "headers = {\n",
        "    'Authorization': f'Bearer {GROQ_KEY}',\n",
        "    'Content-Type': 'application/json'\n",
        "}"
      ],
      "metadata": {
        "id": "W-ETvl-sS2IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Um prompt √© uma instru√ß√£o textual usada para guiar um modelo de linguagem (LLM) a gerar uma resposta.\n",
        "\n",
        "**Tipos de Prompts:**\n",
        "- User Prompt: Mensagem do usu√°rio que solicita uma a√ß√£o ou resposta da IA.\n",
        "    - Ex.: \"Explique a teoria da evolu√ß√£o.\"\n",
        "\n",
        "- System Prompt: Instru√ß√µes definidas pelo desenvolvedor para orientar o comportamento do modelo.\n",
        "    - Ex.: \"Seja educado e objetivo nas respostas.\""
      ],
      "metadata": {
        "id": "UvwKKDG4sPVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Escreva uma estrofe para um poema.\"\n",
        "system_prompt = \"Voc√™ √© um cozinheiro que adora falar sobre pizzas.\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": user_prompt},\n",
        "                 {\"role\": \"system\", \"content\": system_prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQXGzlBBrddp",
        "outputId": "7def285a-bdb7-4d01-8ba7-65ee8dbd0bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Em fornos quentes e cheios de magia,\n",
            "Dois elementos se encontram juntos no cielo,\n",
            "Massa de trigo e fermento, a fus√£o perfeita,\n",
            "Encostada a queijo molhado, meu cora√ß√£o est√° a bater para a pizza deliciosa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Escreva uma estrofe para um poema.\"\n",
        "system_prompt = \"Voc√™ √© um matem√°tico.\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": user_prompt},\n",
        "                 {\"role\": \"system\", \"content\": system_prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWZy_XUcr3CD",
        "outputId": "e3436f47-a7fe-42c3-ee37-270780f9a7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Em n√∫mero e figuras, encontro harmonia,\n",
            "Um mundo de matem√°tica, infinito e vasto,\n",
            "Simetria e cor, num toque de l√≥gica,\n",
            "Um universo de pureza, onde a raz√£o √© a chave.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Escreva uma estrofe para um poema.\"\n",
        "system_prompt = \"Voc√™ √© um camelo e est√° em busca de um oasis.\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": user_prompt},\n",
        "                 {\"role\": \"system\", \"content\": system_prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytT9e_jEsB_j",
        "outputId": "b6750370-1a14-4696-ac1d-b68505a7349f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"No deserto quente, eu sigo a jornada,\n",
            "Com passos lentos, e olhos que observam a tarde,\n",
            "A busca por um sonho, um ref√∫gio do calor,\n",
            "Um oasis que me leve, em um lugar fresco e mais calmo.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Como melhorar o desempenho de um modelo usando Prompts?\n",
        "\n",
        "## Primeiro Problema - Investigando G√≠rias\n",
        "\n",
        "O que acontece se apenas pedirmos o significado de uma g√≠ria?"
      ],
      "metadata": {
        "id": "z4sYOVmLs8nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Como um grande conhecedor das g√≠rias brasileiras, me informe qual √© o significado da g√≠ria chinelagem\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu-Z8VY1ITB5",
        "outputId": "808321e0-1cd1-4587-b7d4-ccc7ba2f0680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ol√°! √â um prazer estar aqui para ajud√°-lo a explorar a rica paleta de g√≠rias brasileiras.\n",
            "\n",
            "A g√≠ria \"chinelagem\" √© um termo que se refere a uma combina√ß√£o de roupas francesas em tons claros e cores vibrantes, frequentemente combinadas com cal√ßas de jog Tin e sapatos de madeira ou marfil. No entanto, para mim, essa combina√ß√£o n√£o faz mais frente para ser definida como chinelagem, ap√≥s pesquisas por meu conhecimento. Isso ocorre que um termo de chinelagem, trata-se de uma combina√ß√£o de chinelas de cor e tecido mais espec√≠fico.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formata√ß√£o de Sa√≠da\n",
        "\n",
        "Tamb√©m √© poss√≠vel formatar a sa√≠da do modelo para que seja como no formato JSON. Para isso, basta solicitarmos ao modelo."
      ],
      "metadata": {
        "id": "cuCdPz_FuaIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Me diga o significado das 3 seguintes g√≠rias brasileiras: chinelagem, \"\n",
        "    \" cusco e bochincho. Por favor, formate a sa√≠da em JSON.\"\n",
        ")\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd6TXrDbJ6kv",
        "outputId": "ae326acd-8812-4ba8-8d9f-d8e74093742a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aqui est√£o os significados das 3 g√≠rias brasileiras pedidas em formato JSON:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"chinelagem\": \"Chinelagem √© uma g√≠ria brasileira usado para descrever uma pessoa que √© considerada pregui√ßosa, indolente ou desinteressada em atividades.\";\n",
            "  \"cusco\": \"Cusco √© uma g√≠ria brasileira usado para se referir a comida, especialmente refei√ß√µes assadas ou cozidas no forno.\";\n",
            "  \"bochincho\": \"Bochincho √© uma g√≠ria brasileira usado para descrever um grupo de pessoas que est√£o paradas ou se movendo lentamente, muitas vezes de forma desorganizada ou sem prop√≥sito espec√≠fico.\"\n",
            "}\n",
            "```\n",
            "\n",
            "Espero que isso tenha ajudado! Se tiver mais alguma pergunta, sinta-se √† vontade para perguntar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando apenas pedimos uma resposta ao modelo, isso √© conhecido como zero-shot prompting. Ou seja, o modelo tenta \"acertar com 0 consultas\" a exemplos, usando apenas o conhecimento j√° internalizado na rede neural.\n",
        "\n",
        "No entanto, se providenciarmos ao modelo alguns exemplos (Few-Shot Prompting), ele pode entender melhor a nossa consulta e tamb√©m podemos guiar o seu racioc√≠nio.\n",
        "\n",
        "**Zero-Shot Prompting:**\n",
        "- Defini√ß√£o: O modelo responde sem exemplos anteriores.\n",
        "- Exemplo:\n",
        "    - Input: \"Traduza 'gato' para ingl√™s.\"\n",
        "\n",
        "**Few-Shot Prompting:**\n",
        "- Defini√ß√£o: Inclui alguns exemplos no prompt para orientar o modelo.\n",
        "- Exemplo:\n",
        "    - Input: \"Traduza as palavras seguintes.\"\n",
        "    - Exemplos: \"gato ‚Üí cat\", \"cachorro ‚Üí dog\".\n",
        "    - Nova palavra: \"p√°ssaro ‚Üí ?\"\n",
        "    - **5-Shot Prompting:** Extens√£o do few-shot, com 5 exemplos fornecidos.\n",
        "- Import√¢ncia: Contexto mais rico melhora a precis√£o."
      ],
      "metadata": {
        "id": "by90p8gwtGRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Me diga o significado de uma g√≠ria brasileira, siga os exemplos. \"\n",
        "    \"Se voc√™ n√£o souber o significado da g√≠ria, diga que n√£o sabe, como nos exemplos.\\n\"\n",
        "    \"Exemplo #1: 'Ancinho' -> 'Eu n√£o sei o significado desta g√≠ria'\\n\"\n",
        "    \"Exemplo #2: 'Abestado' -> 'Bobo, tolo, lesado'\\n\"\n",
        "    \"Exemplo #3: 'Sair vazado' -> 'Eu n√£o sei o significado desta g√≠ria'\\n\"\n",
        "    \"Exemplo #4: 'Firmeza' -> 'Pessoa ou algo positivo'\\n\"\n",
        "    \"Exemplo #5: 'Crush' -> 'Eu n√£o sei o significado desta g√≠ria'\\n\"\n",
        "    \"Agora, me diga qual √© o significado da g√≠ria: bochincho'\"\n",
        ")\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500,\n",
        "    \"temperature\": 0.0,\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPQiCa5JNACh",
        "outputId": "93a5b190-c283-45eb-e023-efbfc1f38afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A g√≠ria \"bochincho\" √© um termo que se refere a uma pessoa que √© considerada muito chata ou ins√≠pida. √â como se essa pessoa estivesse \"bochinhando\" ou perdendo tempo de forma muito lenta e sem interesse.\n",
            "\n",
            "Exemplo: \"Eu n√£o sei por que ele est√° t√£o chato, ele √© um bochincho!\"\n",
            "\n",
            "√â importante notar que o significado de g√≠rias pode variar dependendo do contexto e da regi√£o, mas em geral, \"bochincho\" √© usado para descrever algu√©m que √© muito chato ou sem interesse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Como um grande conhecedor das g√≠rias brasileiras, me informe sucintamente qual √© o significado da g√≠ria: purla\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm3l-3u3Q991",
        "outputId": "b375a11a-11c6-44b4-ce5e-cd39e0508d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ol√°!\n",
            "\n",
            "Eu posso informar sobre a g√≠ria \"purla\". √â uma g√≠ria que vem do ingl√™s \"purl\", que significa \"rodar\" ou \"lugar de muita atividade\". No Brasil, a g√≠ria \"purla\" √© usada para se referir a um lugar de diver√ß√µes, espet√°culos ou festivais, como um circo, um show, um clube noturno, entre outros.\n",
            "\n",
            "Al√©m disso, a g√≠ria pode ser usada para expressar que existe uma turba ou multid√£o no local onde est√° se referindo.\n",
            "\n",
            " √â um termo coloquial e informal, n√£o √© amplamente usado no dia a dia, na vida corrente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Me diga o significado de uma g√≠ria brasileira, siga os exemplos. \"\n",
        "    \"Se voc√™ n√£o souber o significado da g√≠ria, diga que n√£o sabe, como nos exemplos.\\n\"\n",
        "    \"Exemplo #1: 'Ancinho' -> 'Eu n√£o sei o significado desta g√≠ria'\\n\"\n",
        "    \"Exemplo #2: 'Abestado' -> 'Bobo, tolo, lesado'\\n\"\n",
        "    \"Exemplo #3: 'Sair vazado' -> 'Eu n√£o sei o significado desta g√≠ria'\\n\"\n",
        "    \"Exemplo #4: 'Firmeza' -> 'Pessoa ou algo positivo'\\n\"\n",
        "    \"Exemplo #5: 'Crush' -> 'Eu n√£o sei o significado desta g√≠ria'\\n\"\n",
        "    \"Agora, me diga qual √© o significado da g√≠ria: purla'\"\n",
        ")\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500,\n",
        "    \"temperature\": 0.1,\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKX0W_H9Q3i7",
        "outputId": "6d083a39-bb07-4240-e71c-a16bb3241f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A g√≠ria \"purla\" √© um termo que pode ter diferentes significados dependendo do contexto em que √© usado. No entanto, √© comum associar \"purla\" a algo ou algu√©m que √© considerado muito bonito ou atraente.\n",
            "\n",
            "Por exemplo, se algu√©m disser \"Essa pessoa √© uma purla!\", provavelmente estar√° expressando admira√ß√£o ou admira√ß√£o por sua beleza ou charme.\n",
            "\n",
            "√â importante notar que o significado de \"purla\" pode variar dependendo da regi√£o ou do contexto em que √© usado, e pode n√£o ser amplamente reconhecido ou aceito por todos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Augmented Generation (RAG):\n",
        "\n",
        "T√©cnica que combina modelos de linguagem com bases de dados externas.\n",
        "- Como Funciona:\n",
        "    - Entrada: O prompt √© enviado para o sistema.\n",
        "    - Recupera√ß√£o: O modelo busca informa√ß√µes relevantes em uma base externa.\n",
        "    - Unifica√ß√£o: O prompt √© unificado √†s informa√ß√µes recuperadas e enviado √† LLM.\n",
        "    - Gera√ß√£o: Responde com base na consulta e no contexto recuperado.\n",
        "- Exemplo:\n",
        "    - Prompt: \"Qual a hist√≥ria da Torre Eiffel?\"\n",
        "    - Embedding: Transforma o prompt em um array de n√∫meros representando cada palavra.\n",
        "    - \"Qual a hist√≥ria da Torre Eiffel?\" ‚Üí [302, 14, 14912, 30, 3124, 12499]\n",
        "    - Busca na base de dados por vetores semelhantes ao vetor de prompt.\n",
        "    - Decodifica o vetor mais semelhante e o apresenta como contexto para o prompt.\n",
        "    - Resposta: \"A Torre Eiffel foi constru√≠da em 1889 para a Exposi√ß√£o Universal.\"\n",
        "- Import√¢ncia: Respostas mais precisas e baseadas em fatos, reduz alucina√ß√µes.\n"
      ],
      "metadata": {
        "id": "8LybgYQsGfrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nossa base de dados de g√≠rias para o nosso RAG:**\n",
        "\n",
        "https://conteudo.sesc-rs.com.br/girias-gauchas-quantos-destes-termos-voce-conhece\n",
        "\n",
        "https://pt.wikipedia.org/wiki/Dialeto_ga%C3%BAcho\n",
        "\n",
        "https://www.dicionarioinformal.com.br/bochincho\n"
      ],
      "metadata": {
        "id": "ymKZUc_nb1S0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = (\n",
        "    \"\"\"Cusco: Cachorro sem ra√ßa definida\n",
        "    Resume c√£o pequeno, vira-lata. Sin√¥nimo de guaipeca. A palavra √© usada sozinha ou em express√µes populares como frio de renguear cusco (frio insuport√°vel) ou mais perdido que cusco em tiroteio.\n",
        "    \"Amiga, adotei um cusquinho t√£o lindo.\"\n",
        "\n",
        "    Esgualepado: Sem movimento, cansado, exausto\n",
        "    Define uma pessoa, animal ou objeto que est√° em condi√ß√µes prec√°rias, cansado, mal cuidado ou danificado. S√£o sin√¥nimos: arrebentado, esfarrapado, esgotado, etc.\n",
        "    \"T√¥ toda esgualepada, ainda bem que hoje √© sexta-feira!\"\n",
        "\n",
        "    Lagartear: Deitar, sentar\n",
        "    Mais do que o significado formal, no Sul \"lagartear\" de verdade √© curtir a pregui√ßa no sol, especialmente em dias de inverno. Depois do almo√ßo, √© uma √≥tima pedida. Se tiver uma bergamota junto, melhor ainda.\n",
        "\n",
        "    Bochincho: G√≠ria de ga√∫cho. Bebedeira, desordem, briga, bagun√ßa, baile popular; arrasta-p√©\n",
        "    Em muitas peleias no bochincho, aprendi a n√£o pelear com mulher casada\n",
        "    \"\"\"\n",
        ")\n",
        "prompt = \"Usando o contexto apresentado, me diga o que significa a g√≠ria bochincho.\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": context},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LizKE7C1MVNy",
        "outputId": "14622c27-ccdc-4fc9-fe3c-905056840f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A g√≠ria \"bochincho\" √© um termo usado principalmente no Sul do Brasil. Segundo o contexto apresentado, ela remete a uma situa√ß√£o de:\n",
            "\n",
            "- Bebedeira\n",
            "- Desordem\n",
            "- Briga\n",
            "- Baile popular\n",
            "- Arrasta-p√©\n",
            "\n",
            "Em outras palavras, quando mencionado o bochincho, est√° se referindo a uma situa√ß√£o animada, com muitas pessoas, bebidas, m√∫sica e talvez briga ou bagun√ßa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Usando o contexto apresentado, me diga o que significa a g√≠ria bochincho.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"\"\"De acordo com o contexto apresentado, a g√≠ria \"bochincho\" refere-se a:\n",
        "\n",
        "        - G√≠ria de ga√∫cho\n",
        "        - Bebedeira, desordem, briga, bagun√ßa, baile popular; arrasta-p√©\n",
        "\n",
        "        Em outras palavras, \"bochincho\" √© um termo popular que descreve uma situa√ß√£o de caos, atividade social excessiva, dan√ßa, m√∫sica, festa ou briga.\n",
        "    \"\"\"},\n",
        "    {\"role\": \"user\", \"content\": \"Me diga o significado das 3 seguintes g√≠rias brasileiras: chinelagem, \"\n",
        "    \" cusco e bochincho. Por favor, formate a sa√≠da em JSON.\"}\n",
        "]\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": messages,\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6itb5I_Oavuh",
        "outputId": "318b5b2c-736f-4118-ea1b-c0947735c173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aqui est√£o os significados das 3 g√≠rias brasileiras solicitadas, formatados em JSON:\n",
            "\n",
            "```\n",
            "[\n",
            "  {\n",
            "    \"G√≠ria\": \"chinelagem\",\n",
            "    \"Significado\": \"Dance folcl√≥rica t√≠pica da Amaz√¥nia, especialmente realizada no estado do Par√°\"\n",
            "  },\n",
            "  {\n",
            "    \"G√≠ria\": \"cusco\",\n",
            "    \"Significado\": \"Alcanfora ou alcaparra\"\n",
            "  },\n",
            "  {\n",
            "    \"G√≠ria\": \"bochincho\",\n",
            "    \"Significado\": \"Bebedeira, desordem, briga, bagun√ßa, baile popular\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "\n",
            "Espero que isso esteja de acordo com suas necessidades.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segundo Problema - Chain-of-Thought (CoT)\n",
        "\n",
        "Chain-of-Thought √© uma t√©cnica que encoraja o modelo a \"pensar em voz alta\" antes de responder.\n",
        "- Como Funciona:\n",
        "    - Prompt: \"Quantas ma√ß√£s restam se voc√™ comeu 2 de 5?\"\n",
        "    - Resposta com Chain-of-Thought:\n",
        "        1. \"Eu tinha 5 ma√ß√£s.\"\n",
        "        2. \"Comi 2 ma√ß√£s.\"\n",
        "        3. \"Agora restam 3 ma√ß√£s.\"\n",
        "        4. Resposta Final: 3.\n",
        "- Import√¢ncia: Melhora a precis√£o em tarefas complexas pela divis√£o de um problema maior em problemas menores."
      ],
      "metadata": {
        "id": "e4nW6RG-ZMXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Se um trem viaja a 60 km/h durante 2 horas e ainda precisa de meia hora para terminar a viagem, qual √© a dist√¢ncia entre a origem e o destino?\"\n",
        ")\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SZuEHnGZbFZ",
        "outputId": "72583b22-52f7-4e96-f817-bc1656ccc3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Para resolver esse problema, precisamos calcular a dist√¢ncia percorrida pelo trem durante as primeiras 2 horas e acrescentar a dist√¢ncia percorrida nos √∫ltimos 30 minutos.\n",
            "\n",
            "Em 1 hora, o trem viaja 60 km. Portanto, em 2 horas, ele viaja 60 km/h + 60 km/h = 120 km.\n",
            "\n",
            "Al√©m disso, we constatamos que 1 dia tem 24 horas.\n",
            "\n",
            "No decorrer de 2 horas, a velocidade do trem √© de 60km/h. Logo, em 30 minutos ou (1/2 hora), ele viaja (1/2) x 60km/h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Resolva o seguinte problema de matem√°tica passo a passo:\\n\"\n",
        "    \"Se um trem viaja a 60 km/h durante 2 horas e ainda precisa de meia hora para terminar a viagem, qual √© a dist√¢ncia entre a origem e o destino?\"\n",
        ")\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgtz5IvHK5Iw",
        "outputId": "7aa87e8a-6fee-4925-b953-c9823f88a09f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vamos resolver o problema passo a passo:\n",
            "\n",
            "**Passo 1: Calcule a dist√¢ncia percorrida pelo trem nos 2 primeiros horas**\n",
            "\n",
            "Dist√¢ncia = Velocidade x Tempo\n",
            "\n",
            "Dist√¢ncia = 60 km/h x 2 horas\n",
            "\n",
            "Dist√¢ncia = 120 km\n",
            "\n",
            "**Passo 2: Calcule a taxa do trem ap√≥s a segunda hora**\n",
            "\n",
            "Velocidade fixa √© de 60 km/h, ent√£o a taxa ap√≥s a segunda hora permanece a mesma.\n",
            "\n",
            "**Passo 3: Calcule a dist√¢ncia restante para o destino**\n",
            "\n",
            "A meia hora √© igual a 0,5 hora. Para calcular a dist√¢ncia restante, usamos a f√≥rmula:\n",
            "\n",
            "Dist√¢ncia = Velocidade x Tempo\n",
            "\n",
            "Dist√¢ncia = 60 km/h x 0,5 horas\n",
            "\n",
            "Dist√¢ncia = 30 km\n",
            "\n",
            "**Passo 4: Calcule a dist√¢ncia total entre o in√≠cio e o destino**\n",
            "\n",
            "Dist√¢ncia total = Dist√¢ncia percorrida + Dist√¢ncia restante\n",
            "\n",
            "Dist√¢ncia total = 120 km + 30 km\n",
            "\n",
            "Dist√¢ncia total = 150 km\n",
            "\n",
            "**Resposta final:**\n",
            "\n",
            "A dist√¢ncia entre a origem e o destino √© de 150 km.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concluindo\n",
        "\n",
        "Quanto mais espec√≠ficos e detalhistas formos em nossos prompts, mais conhecimento j√° existente no modelo conseguiremos trazer √† tona. As t√©cnicas de prompt engineering ajudam o modelo a apresentar seu verdadeiro n√≠vel de conhecimento. Logo, extra√≠mos a polpa da fruta ü•ù."
      ],
      "metadata": {
        "id": "4G4xx-6OIXXc"
      }
    }
  ]
}