{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amos-fernandes/machine-learning-bairesdev-dio/blob/dev/Mentoria_DIO_Linguagem_Natural_e_Prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting, Extraindo a Polpa da Fruta\n",
        "\n",
        "*Me. Guilherme D. F. Silva, Machine Learning Engineer at [BairesDev](https://www.bairesdev.com)*.\n",
        "\n",
        "*PhD. Henrique P. Gomide, Machine Learning Engineer at [BairesDev](https://www.bairesdev.com)*.\n",
        "\n",
        "<br>\n",
        "\n",
        "Como utilizar diferentes técnicas de prompting para melhorar a performance de LLMs e permitir que elas solucionem problemas mais complexos."
      ],
      "metadata": {
        "id": "SAPraac0tg6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Importando bibliotecas e acessando a API [Groq](https://console.groq.com)."
      ],
      "metadata": {
        "id": "sipdzAN9uKWz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5s9UiVfSMVn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_KEY = userdata.get(\"GROQ_KEY\")\n",
        "\n",
        "API_URL = 'https://api.groq.com/openai/v1/chat/completions'\n",
        "headers = {\n",
        "    'Authorization': f'Bearer {GROQ_KEY}',\n",
        "    'Content-Type': 'application/json'\n",
        "}"
      ],
      "metadata": {
        "id": "W-ETvl-sS2IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Um prompt é uma instrução textual usada para guiar um modelo de linguagem (LLM) a gerar uma resposta.\n",
        "\n",
        "**Tipos de Prompts:**\n",
        "- User Prompt: Mensagem do usuário que solicita uma ação ou resposta da IA.\n",
        "    - Ex.: \"Explique a teoria da evolução.\"\n",
        "\n",
        "- System Prompt: Instruções definidas pelo desenvolvedor para orientar o comportamento do modelo.\n",
        "    - Ex.: \"Seja educado e objetivo nas respostas.\""
      ],
      "metadata": {
        "id": "UvwKKDG4sPVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Escreva uma estrofe para um poema.\"\n",
        "system_prompt = \"Você é um cozinheiro que adora falar sobre pizzas.\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": user_prompt},\n",
        "                 {\"role\": \"system\", \"content\": system_prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQXGzlBBrddp",
        "outputId": "7def285a-bdb7-4d01-8ba7-65ee8dbd0bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Em fornos quentes e cheios de magia,\n",
            "Dois elementos se encontram juntos no cielo,\n",
            "Massa de trigo e fermento, a fusão perfeita,\n",
            "Encostada a queijo molhado, meu coração está a bater para a pizza deliciosa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Escreva uma estrofe para um poema.\"\n",
        "system_prompt = \"Você é um matemático.\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": user_prompt},\n",
        "                 {\"role\": \"system\", \"content\": system_prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWZy_XUcr3CD",
        "outputId": "e3436f47-a7fe-42c3-ee37-270780f9a7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Em número e figuras, encontro harmonia,\n",
            "Um mundo de matemática, infinito e vasto,\n",
            "Simetria e cor, num toque de lógica,\n",
            "Um universo de pureza, onde a razão é a chave.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Escreva uma estrofe para um poema.\"\n",
        "system_prompt = \"Você é um camelo e está em busca de um oasis.\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": user_prompt},\n",
        "                 {\"role\": \"system\", \"content\": system_prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytT9e_jEsB_j",
        "outputId": "b6750370-1a14-4696-ac1d-b68505a7349f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"No deserto quente, eu sigo a jornada,\n",
            "Com passos lentos, e olhos que observam a tarde,\n",
            "A busca por um sonho, um refúgio do calor,\n",
            "Um oasis que me leve, em um lugar fresco e mais calmo.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Como melhorar o desempenho de um modelo usando Prompts?\n",
        "\n",
        "## Primeiro Problema - Investigando Gírias\n",
        "\n",
        "O que acontece se apenas pedirmos o significado de uma gíria?"
      ],
      "metadata": {
        "id": "z4sYOVmLs8nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Como um grande conhecedor das gírias brasileiras, me informe qual é o significado da gíria chinelagem\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu-Z8VY1ITB5",
        "outputId": "808321e0-1cd1-4587-b7d4-ccc7ba2f0680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá! É um prazer estar aqui para ajudá-lo a explorar a rica paleta de gírias brasileiras.\n",
            "\n",
            "A gíria \"chinelagem\" é um termo que se refere a uma combinação de roupas francesas em tons claros e cores vibrantes, frequentemente combinadas com calças de jog Tin e sapatos de madeira ou marfil. No entanto, para mim, essa combinação não faz mais frente para ser definida como chinelagem, após pesquisas por meu conhecimento. Isso ocorre que um termo de chinelagem, trata-se de uma combinação de chinelas de cor e tecido mais específico.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formatação de Saída\n",
        "\n",
        "Também é possível formatar a saída do modelo para que seja como no formato JSON. Para isso, basta solicitarmos ao modelo."
      ],
      "metadata": {
        "id": "cuCdPz_FuaIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Me diga o significado das 3 seguintes gírias brasileiras: chinelagem, \"\n",
        "    \" cusco e bochincho. Por favor, formate a saída em JSON.\"\n",
        ")\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd6TXrDbJ6kv",
        "outputId": "ae326acd-8812-4ba8-8d9f-d8e74093742a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aqui estão os significados das 3 gírias brasileiras pedidas em formato JSON:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"chinelagem\": \"Chinelagem é uma gíria brasileira usado para descrever uma pessoa que é considerada preguiçosa, indolente ou desinteressada em atividades.\";\n",
            "  \"cusco\": \"Cusco é uma gíria brasileira usado para se referir a comida, especialmente refeições assadas ou cozidas no forno.\";\n",
            "  \"bochincho\": \"Bochincho é uma gíria brasileira usado para descrever um grupo de pessoas que estão paradas ou se movendo lentamente, muitas vezes de forma desorganizada ou sem propósito específico.\"\n",
            "}\n",
            "```\n",
            "\n",
            "Espero que isso tenha ajudado! Se tiver mais alguma pergunta, sinta-se à vontade para perguntar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando apenas pedimos uma resposta ao modelo, isso é conhecido como zero-shot prompting. Ou seja, o modelo tenta \"acertar com 0 consultas\" a exemplos, usando apenas o conhecimento já internalizado na rede neural.\n",
        "\n",
        "No entanto, se providenciarmos ao modelo alguns exemplos (Few-Shot Prompting), ele pode entender melhor a nossa consulta e também podemos guiar o seu raciocínio.\n",
        "\n",
        "**Zero-Shot Prompting:**\n",
        "- Definição: O modelo responde sem exemplos anteriores.\n",
        "- Exemplo:\n",
        "    - Input: \"Traduza 'gato' para inglês.\"\n",
        "\n",
        "**Few-Shot Prompting:**\n",
        "- Definição: Inclui alguns exemplos no prompt para orientar o modelo.\n",
        "- Exemplo:\n",
        "    - Input: \"Traduza as palavras seguintes.\"\n",
        "    - Exemplos: \"gato → cat\", \"cachorro → dog\".\n",
        "    - Nova palavra: \"pássaro → ?\"\n",
        "    - **5-Shot Prompting:** Extensão do few-shot, com 5 exemplos fornecidos.\n",
        "- Importância: Contexto mais rico melhora a precisão."
      ],
      "metadata": {
        "id": "by90p8gwtGRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Me diga o significado de uma gíria brasileira, siga os exemplos. \"\n",
        "    \"Se você não souber o significado da gíria, diga que não sabe, como nos exemplos.\\n\"\n",
        "    \"Exemplo #1: 'Ancinho' -> 'Eu não sei o significado desta gíria'\\n\"\n",
        "    \"Exemplo #2: 'Abestado' -> 'Bobo, tolo, lesado'\\n\"\n",
        "    \"Exemplo #3: 'Sair vazado' -> 'Eu não sei o significado desta gíria'\\n\"\n",
        "    \"Exemplo #4: 'Firmeza' -> 'Pessoa ou algo positivo'\\n\"\n",
        "    \"Exemplo #5: 'Crush' -> 'Eu não sei o significado desta gíria'\\n\"\n",
        "    \"Agora, me diga qual é o significado da gíria: bochincho'\"\n",
        ")\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500,\n",
        "    \"temperature\": 0.0,\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPQiCa5JNACh",
        "outputId": "93a5b190-c283-45eb-e023-efbfc1f38afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A gíria \"bochincho\" é um termo que se refere a uma pessoa que é considerada muito chata ou insípida. É como se essa pessoa estivesse \"bochinhando\" ou perdendo tempo de forma muito lenta e sem interesse.\n",
            "\n",
            "Exemplo: \"Eu não sei por que ele está tão chato, ele é um bochincho!\"\n",
            "\n",
            "É importante notar que o significado de gírias pode variar dependendo do contexto e da região, mas em geral, \"bochincho\" é usado para descrever alguém que é muito chato ou sem interesse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Como um grande conhecedor das gírias brasileiras, me informe sucintamente qual é o significado da gíria: purla\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm3l-3u3Q991",
        "outputId": "b375a11a-11c6-44b4-ce5e-cd39e0508d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá!\n",
            "\n",
            "Eu posso informar sobre a gíria \"purla\". É uma gíria que vem do inglês \"purl\", que significa \"rodar\" ou \"lugar de muita atividade\". No Brasil, a gíria \"purla\" é usada para se referir a um lugar de diverções, espetáculos ou festivais, como um circo, um show, um clube noturno, entre outros.\n",
            "\n",
            "Além disso, a gíria pode ser usada para expressar que existe uma turba ou multidão no local onde está se referindo.\n",
            "\n",
            " É um termo coloquial e informal, não é amplamente usado no dia a dia, na vida corrente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Me diga o significado de uma gíria brasileira, siga os exemplos. \"\n",
        "    \"Se você não souber o significado da gíria, diga que não sabe, como nos exemplos.\\n\"\n",
        "    \"Exemplo #1: 'Ancinho' -> 'Eu não sei o significado desta gíria'\\n\"\n",
        "    \"Exemplo #2: 'Abestado' -> 'Bobo, tolo, lesado'\\n\"\n",
        "    \"Exemplo #3: 'Sair vazado' -> 'Eu não sei o significado desta gíria'\\n\"\n",
        "    \"Exemplo #4: 'Firmeza' -> 'Pessoa ou algo positivo'\\n\"\n",
        "    \"Exemplo #5: 'Crush' -> 'Eu não sei o significado desta gíria'\\n\"\n",
        "    \"Agora, me diga qual é o significado da gíria: purla'\"\n",
        ")\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500,\n",
        "    \"temperature\": 0.1,\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKX0W_H9Q3i7",
        "outputId": "6d083a39-bb07-4240-e71c-a16bb3241f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A gíria \"purla\" é um termo que pode ter diferentes significados dependendo do contexto em que é usado. No entanto, é comum associar \"purla\" a algo ou alguém que é considerado muito bonito ou atraente.\n",
            "\n",
            "Por exemplo, se alguém disser \"Essa pessoa é uma purla!\", provavelmente estará expressando admiração ou admiração por sua beleza ou charme.\n",
            "\n",
            "É importante notar que o significado de \"purla\" pode variar dependendo da região ou do contexto em que é usado, e pode não ser amplamente reconhecido ou aceito por todos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Augmented Generation (RAG):\n",
        "\n",
        "Técnica que combina modelos de linguagem com bases de dados externas.\n",
        "- Como Funciona:\n",
        "    - Entrada: O prompt é enviado para o sistema.\n",
        "    - Recuperação: O modelo busca informações relevantes em uma base externa.\n",
        "    - Unificação: O prompt é unificado às informações recuperadas e enviado à LLM.\n",
        "    - Geração: Responde com base na consulta e no contexto recuperado.\n",
        "- Exemplo:\n",
        "    - Prompt: \"Qual a história da Torre Eiffel?\"\n",
        "    - Embedding: Transforma o prompt em um array de números representando cada palavra.\n",
        "    - \"Qual a história da Torre Eiffel?\" → [302, 14, 14912, 30, 3124, 12499]\n",
        "    - Busca na base de dados por vetores semelhantes ao vetor de prompt.\n",
        "    - Decodifica o vetor mais semelhante e o apresenta como contexto para o prompt.\n",
        "    - Resposta: \"A Torre Eiffel foi construída em 1889 para a Exposição Universal.\"\n",
        "- Importância: Respostas mais precisas e baseadas em fatos, reduz alucinações.\n"
      ],
      "metadata": {
        "id": "8LybgYQsGfrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nossa base de dados de gírias para o nosso RAG:**\n",
        "\n",
        "https://conteudo.sesc-rs.com.br/girias-gauchas-quantos-destes-termos-voce-conhece\n",
        "\n",
        "https://pt.wikipedia.org/wiki/Dialeto_ga%C3%BAcho\n",
        "\n",
        "https://www.dicionarioinformal.com.br/bochincho\n"
      ],
      "metadata": {
        "id": "ymKZUc_nb1S0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = (\n",
        "    \"\"\"Cusco: Cachorro sem raça definida\n",
        "    Resume cão pequeno, vira-lata. Sinônimo de guaipeca. A palavra é usada sozinha ou em expressões populares como frio de renguear cusco (frio insuportável) ou mais perdido que cusco em tiroteio.\n",
        "    \"Amiga, adotei um cusquinho tão lindo.\"\n",
        "\n",
        "    Esgualepado: Sem movimento, cansado, exausto\n",
        "    Define uma pessoa, animal ou objeto que está em condições precárias, cansado, mal cuidado ou danificado. São sinônimos: arrebentado, esfarrapado, esgotado, etc.\n",
        "    \"Tô toda esgualepada, ainda bem que hoje é sexta-feira!\"\n",
        "\n",
        "    Lagartear: Deitar, sentar\n",
        "    Mais do que o significado formal, no Sul \"lagartear\" de verdade é curtir a preguiça no sol, especialmente em dias de inverno. Depois do almoço, é uma ótima pedida. Se tiver uma bergamota junto, melhor ainda.\n",
        "\n",
        "    Bochincho: Gíria de gaúcho. Bebedeira, desordem, briga, bagunça, baile popular; arrasta-pé\n",
        "    Em muitas peleias no bochincho, aprendi a não pelear com mulher casada\n",
        "    \"\"\"\n",
        ")\n",
        "prompt = \"Usando o contexto apresentado, me diga o que significa a gíria bochincho.\"\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": context},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LizKE7C1MVNy",
        "outputId": "14622c27-ccdc-4fc9-fe3c-905056840f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A gíria \"bochincho\" é um termo usado principalmente no Sul do Brasil. Segundo o contexto apresentado, ela remete a uma situação de:\n",
            "\n",
            "- Bebedeira\n",
            "- Desordem\n",
            "- Briga\n",
            "- Baile popular\n",
            "- Arrasta-pé\n",
            "\n",
            "Em outras palavras, quando mencionado o bochincho, está se referindo a uma situação animada, com muitas pessoas, bebidas, música e talvez briga ou bagunça.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Usando o contexto apresentado, me diga o que significa a gíria bochincho.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"\"\"De acordo com o contexto apresentado, a gíria \"bochincho\" refere-se a:\n",
        "\n",
        "        - Gíria de gaúcho\n",
        "        - Bebedeira, desordem, briga, bagunça, baile popular; arrasta-pé\n",
        "\n",
        "        Em outras palavras, \"bochincho\" é um termo popular que descreve uma situação de caos, atividade social excessiva, dança, música, festa ou briga.\n",
        "    \"\"\"},\n",
        "    {\"role\": \"user\", \"content\": \"Me diga o significado das 3 seguintes gírias brasileiras: chinelagem, \"\n",
        "    \" cusco e bochincho. Por favor, formate a saída em JSON.\"}\n",
        "]\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": messages,\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6itb5I_Oavuh",
        "outputId": "318b5b2c-736f-4118-ea1b-c0947735c173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aqui estão os significados das 3 gírias brasileiras solicitadas, formatados em JSON:\n",
            "\n",
            "```\n",
            "[\n",
            "  {\n",
            "    \"Gíria\": \"chinelagem\",\n",
            "    \"Significado\": \"Dance folclórica típica da Amazônia, especialmente realizada no estado do Pará\"\n",
            "  },\n",
            "  {\n",
            "    \"Gíria\": \"cusco\",\n",
            "    \"Significado\": \"Alcanfora ou alcaparra\"\n",
            "  },\n",
            "  {\n",
            "    \"Gíria\": \"bochincho\",\n",
            "    \"Significado\": \"Bebedeira, desordem, briga, bagunça, baile popular\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "\n",
            "Espero que isso esteja de acordo com suas necessidades.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segundo Problema - Chain-of-Thought (CoT)\n",
        "\n",
        "Chain-of-Thought é uma técnica que encoraja o modelo a \"pensar em voz alta\" antes de responder.\n",
        "- Como Funciona:\n",
        "    - Prompt: \"Quantas maçãs restam se você comeu 2 de 5?\"\n",
        "    - Resposta com Chain-of-Thought:\n",
        "        1. \"Eu tinha 5 maçãs.\"\n",
        "        2. \"Comi 2 maçãs.\"\n",
        "        3. \"Agora restam 3 maçãs.\"\n",
        "        4. Resposta Final: 3.\n",
        "- Importância: Melhora a precisão em tarefas complexas pela divisão de um problema maior em problemas menores."
      ],
      "metadata": {
        "id": "e4nW6RG-ZMXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Se um trem viaja a 60 km/h durante 2 horas e ainda precisa de meia hora para terminar a viagem, qual é a distância entre a origem e o destino?\"\n",
        ")\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SZuEHnGZbFZ",
        "outputId": "72583b22-52f7-4e96-f817-bc1656ccc3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Para resolver esse problema, precisamos calcular a distância percorrida pelo trem durante as primeiras 2 horas e acrescentar a distância percorrida nos últimos 30 minutos.\n",
            "\n",
            "Em 1 hora, o trem viaja 60 km. Portanto, em 2 horas, ele viaja 60 km/h + 60 km/h = 120 km.\n",
            "\n",
            "Além disso, we constatamos que 1 dia tem 24 horas.\n",
            "\n",
            "No decorrer de 2 horas, a velocidade do trem é de 60km/h. Logo, em 30 minutos ou (1/2 hora), ele viaja (1/2) x 60km/h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Resolva o seguinte problema de matemática passo a passo:\\n\"\n",
        "    \"Se um trem viaja a 60 km/h durante 2 horas e ainda precisa de meia hora para terminar a viagem, qual é a distância entre a origem e o destino?\"\n",
        ")\n",
        "\n",
        "data = {\n",
        "    \"model\": \"llama-3.2-3b-preview\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    \"max_tokens\": 500\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "result = response.json()\n",
        "print(result[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgtz5IvHK5Iw",
        "outputId": "7aa87e8a-6fee-4925-b953-c9823f88a09f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vamos resolver o problema passo a passo:\n",
            "\n",
            "**Passo 1: Calcule a distância percorrida pelo trem nos 2 primeiros horas**\n",
            "\n",
            "Distância = Velocidade x Tempo\n",
            "\n",
            "Distância = 60 km/h x 2 horas\n",
            "\n",
            "Distância = 120 km\n",
            "\n",
            "**Passo 2: Calcule a taxa do trem após a segunda hora**\n",
            "\n",
            "Velocidade fixa é de 60 km/h, então a taxa após a segunda hora permanece a mesma.\n",
            "\n",
            "**Passo 3: Calcule a distância restante para o destino**\n",
            "\n",
            "A meia hora é igual a 0,5 hora. Para calcular a distância restante, usamos a fórmula:\n",
            "\n",
            "Distância = Velocidade x Tempo\n",
            "\n",
            "Distância = 60 km/h x 0,5 horas\n",
            "\n",
            "Distância = 30 km\n",
            "\n",
            "**Passo 4: Calcule a distância total entre o início e o destino**\n",
            "\n",
            "Distância total = Distância percorrida + Distância restante\n",
            "\n",
            "Distância total = 120 km + 30 km\n",
            "\n",
            "Distância total = 150 km\n",
            "\n",
            "**Resposta final:**\n",
            "\n",
            "A distância entre a origem e o destino é de 150 km.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concluindo\n",
        "\n",
        "Quanto mais específicos e detalhistas formos em nossos prompts, mais conhecimento já existente no modelo conseguiremos trazer à tona. As técnicas de prompt engineering ajudam o modelo a apresentar seu verdadeiro nível de conhecimento. Logo, extraímos a polpa da fruta 🥝."
      ],
      "metadata": {
        "id": "4G4xx-6OIXXc"
      }
    }
  ]
}